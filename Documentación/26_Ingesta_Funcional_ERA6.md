# üìÑ Informe Funcional: Flujo de Ingesta ERA 6
**Versi√≥n:** 1.1 (ERA 6 Compatible)
**Fecha:** 20 de febrero de 2026 (Actualizado)
**Estado:** Documentaci√≥n T√©cnica para Decisi√≥n de Negocio

---

## 1. Introducci√≥n
Este documento describe el comportamiento funcional del sistema de ingesta de documentos de la plataforma ABD RAG. El objetivo es aclarar c√≥mo se procesan los datos seg√∫n el modo seleccionado y por qu√© el usuario percibe comportamientos distintos entre ellas.

## 2. Puntos de Entrada y Roles
La ingesta se centraliza en el componente `UnifiedIngestModal`.

*   **¬øQui√©n puede lanzar la ingesta?**
    *   **Administradores (ADMIN)**: Ingesta restringida a su propio **Tenant** (empresa).
    *   **SuperAdministradores (SUPER_ADMIN)**: Pueden elegir el **Alcance (Scope)**: Tenant local, Industria (Global para ascensores) o Global absoluto.
*   **Identidad del Proceso:**
    *   El sistema asocia el documento al `tenantId` del usuario que lo sube.
    *   En los logs t√©cnicos, el proceso as√≠ncrono aparece como realizado por `system_worker`, pero siempre hereda los permisos y el contexto del usuario original.

---

## 3. Los Dos Caminos de Ingesta

La plataforma divide la ingesta en dos filosof√≠as de uso:

### 3.1 Modo Simplificado (Simple Mode)
Es el modo por defecto. Busca el "Time-to-First-Value" (TTFV) en menos de 60 segundos.

*   **Comportamiento Funcional:**
    *   **Auto-Configuraci√≥n (`useSmartConfig`)**: El sistema analiza el nombre y tama√±o del archivo antes de subirlo. 
        *   Si es un PDF peque√±o (<2MB): Configura nivel **Bajo**.
        *   Si es un documento con palabras clave como "contrato" o "legal": Sube autom√°ticamente a nivel **Alto** (Expert).
        *   Si es un PDF grande (>2MB): Configura nivel **Medio**.
    *   **Bypass de LLM en Fragmentaci√≥n (Ahorro de Costes)**: Por defecto, si el nivel es **Bajo**, el sistema **NO usa Inteligencia Artificial** para fragmentar el texto. Simplemente lo trocea por caracteres (1500 caracteres, 200 de solapamiento).
    *   **‚ö†Ô∏è Hallazgo Cr√≠tico: Llamada LLM Oculta**: Aunque la fragmentaci√≥n es mec√°nica, el sistema **S√ç llama a Gemini** para detectar la industria del documento (`DomainRouterService.detectIndustry`). Esto contradice el objetivo de "zero LLM" en modo simple. Ver **Secci√≥n 6, Mejora M-002**.
    *   **‚ö†Ô∏è Protecci√≥n de Datos (PII)**: Actualmente, el sistema puede activar autom√°ticamente la m√°scara de datos sensibles. Esto implica procesamiento adicional que **no deber√≠a ejecutarse** en modo simple sin confirmaci√≥n del usuario. Ver **Secci√≥n 6, Mejora M-004**.
    *   **Funcionalidades Premium**: Est√°n todas **DESACTIVADAS** (No hay visi√≥n de im√°genes, no hay traducci√≥n, no hay extracci√≥n de grafos).

### 3.2 Modo Experto (Expert Mode)
Se activa mediante un interruptor (switch) en la interfaz. Permite control total sobre el pipeline.

*   **Niveles de Chunking (Fragmentaci√≥n):**
    1.  **Bajo (Mec√°nico)**: Fragmentaci√≥n fija por caracteres. Muy r√°pido, pero puede "cortar" frases por la mitad. **Sin coste de LLM.**
    2.  **Medio (Sem√°ntico)**: Divide el texto en oraciones, genera un **embedding** (vector num√©rico v√≠a API de Gemini `text-embedding-004`) para cada una, y agrupa las oraciones por similitud de coseno. Cuando la similitud cae por debajo de un umbral (por defecto 0.75), corta el fragmento. **Consumo moderado de API de Embeddings** (una llamada por oraci√≥n). No genera texto con LLM, pero s√≠ consume tokens de la API.
    3.  **Alto (Ag√©ntico)**: Un agente de IA lee el documento y decide c√≥mo agrupar la informaci√≥n seg√∫n el contenido. **Alto consumo de LLM.** ‚Üí **Funcionalidad PREMIUM** (ligada a facturaci√≥n y plan del cliente). Ver **Secci√≥n 6, Mejora M-006**.
*   **Funcionalidades Premium (Opcionales):**
    *   **Vision**: Analiza im√°genes, planos y fotos dentro del PDF.
    *   **Auto-Traducci√≥n**: Si el documento est√° en otro idioma, lo traduce al espa√±ol durante la ingesta.
    *   **Enriquecimiento Grafo (GraphRAG)**: Extrae entidades y relaciones para b√∫squedas m√°s complejas.
    *   **Recuperaci√≥n Contextual (Cognitive)**: Genera micro-res√∫menes para cada fragmento, lo que mejora dr√°sticamente la calidad de las respuestas posteriores.

---

## 4. ¬øPor qu√© el sistema se comporta "diferente" de lo esperado?

Basado en la arquitectura actual, aqu√≠ est√°n las razones de posibles inconsistencias:

1.  **Modo Simple != Calidad M√°xima**: Al usar el modo simple con documentos peque√±os, el sistema prioriza la velocidad y el ahorro. Si el usuario espera que la IA "entienda" diagramas en modo simple, fallar√° porque el **An√°lisis Visual (Vision)** est√° apagado.
2.  **El "Bypass" del Onboarding**: Durante el onboarding, el sistema fuerza una configuraci√≥n optimizada para que el usuario reciba una respuesta r√°pida, lo que puede diferir de una ingesta manual donde el usuario no activa las opciones premium.
3.  **Aislamiento de Tenant**: Un documento subido como "Global" por un SuperAdmin ser√° visible para todos, pero un documento subido por un Administrador normal solo existir√° para su empresa. Si se busca un documento "perdido", es probable que el **Scope** no fuera el correcto.

## 5. Resumen Comparativo

| Caracter√≠stica | Simple (Bajo) | Experto (Medio) | Experto (Alto) |
| :--- | :--- | :--- | :--- |
| **Fragmentaci√≥n** | Mec√°nica (caracteres) | Sem√°ntica (embeddings) | Ag√©ntica (LLM decide) |
| **Uso de LLM Generativo** | ‚ö†Ô∏è 1 llamada oculta (Industria) | Ninguno directo | Intensivo |
| **Uso de API Embeddings** | Solo al indexar | Alto (1 por oraci√≥n + indexar) | Solo al indexar |
| **Velocidad** | Instant√°nea (1-5s) | Moderada (10-30s) | Lenta (30s - 3min) |
| **Calidad RAG** | Est√°ndar | Buena (cohesi√≥n tem√°tica) | Superior (Contextual) |
| **Determinismo** | ‚ö†Ô∏è Casi total (excepto industria) | Determinista (mismo embedding = mismo corte) | No determinista |
| **Im√°genes/Tablas** | Ignoradas | Ignoradas (salvo activar Vision) | Analizadas (si se activa Vision) |
| **Costo** | Bajo (pero no zero) | Moderado | Alto (Premium) |
| **Configuraci√≥n** | Autom√°tica | Manual | Manual detallada |

---

## 6. Mejoras Propuestas (Pendientes de An√°lisis)

Esta secci√≥n recoge mejoras identificadas durante la auditor√≠a funcional. No est√°n desarrolladas, solo enunciadas para su posterior an√°lisis y priorizaci√≥n.

### M-001: Externalizar Palabras Clave de Auto-Configuraci√≥n

**Estado Actual:** Las palabras clave que disparan el cambio autom√°tico de nivel en Modo Simple est√°n hardcodeadas en `useSmartConfig.ts` ("contrato", "legal", etc.).

**Propuesta:** Mover a un sistema configurable con jerarqu√≠a de alcance:

| Nivel | Ejemplo | Gesti√≥n |
| :--- | :--- | :--- |
| **Global** | Palabras universales ("contrato", "anexo") | Solo SUPER_ADMIN |
| **Por Industria** | Palabras de ascensores ("cuadro", "motor", "foso") | SUPER_ADMIN |
| **Por Tenant** | Palabras espec√≠ficas del cliente ("Schindler", "modelo X") | ADMIN del tenant |

**Opciones de Implementaci√≥n (a evaluar):**
*   Base de datos con capa de cach√© (similar al patr√≥n de `PromptService`).
*   Volcado peri√≥dico a archivo JSON para consumo r√°pido desde el frontend.
*   Modelo jer√°rquico similar a los Spaces (general ‚Üí industria ‚Üí tenant).

**Nota:** No se contempla nivel "por usuario" por ahora.

---

### M-002: Modo Simple = Zero LLM (Eliminar llamada oculta a Industria)

**Estado Actual:** Incluso en Modo Simple, el sistema llama a `DomainRouterService.detectIndustry()`, que internamente usa Gemini para clasificar el documento.

**Propuesta:**
*   En Modo Simple, la industria debe leerse del **contexto de la sesi√≥n** (el tenant ya tiene una industria asociada).
*   Si el usuario es **SUPER_ADMIN** (que puede gestionar m√∫ltiples industrias), se le debe preguntar expl√≠citamente o permitir **reclasificar** el documento despu√©s de la ingesta.
*   Evaluar si es necesario un **"Modo Intermedio"** que permita auto-detecci√≥n de industria sin activar fragmentaci√≥n sem√°ntica completa.

**Objetivo:** Que el Modo Simple sea **100% determinista y offline** (sin llamadas externas a APIs de IA durante la fragmentaci√≥n ni la clasificaci√≥n).

---

### M-003: Revisar Dependencia de Embeddings en Nivel Medio (Sem√°ntico)

**Estado Actual:** El `SemanticChunker` llama a `generateEmbedding()` (API de Gemini `text-embedding-004`) para **cada oraci√≥n** del documento. En un documento de 200 oraciones, son 200 llamadas a la API.

**Problema de Cuotas:** El LLM (Gemini) no solo limita por tokens, sino tambi√©n por:
*   **N√∫mero de llamadas por minuto (RPM)**: 200 oraciones = potencialmente 200 requests en r√°faga.
*   **Tokens por minuto (TPM)**: Documentos grandes pueden agotar la cuota r√°pidamente.
*   **Timeouts**: Cada llamada individual puede fallar por latencia, multiplicando el riesgo.

**Hallazgo:** El proyecto **ya tiene BGE-M3 integrado** como modelo local (`@xenova/transformers` en `multilingual-service.ts`). Su uso est√° controlado por la variable de entorno `ENABLE_LOCAL_EMBEDDINGS`. **Es una alternativa real y viable** para generar embeddings sin depender de la API de Gemini.

**Propuesta:** Evaluar dos sub-niveles para Medio:

| Sub-Nivel | Fragmentaci√≥n | Embeddings | Coste API | Determinismo |
| :--- | :--- | :--- | :--- | :--- |
| **Medio-Local** | Sem√°ntica (coseno) | BGE-M3 (local, offline) | Zero | Alto (modelo local = siempre igual) |
| **Medio-Cloud** | Sem√°ntica (coseno) | Gemini `text-embedding-004` | Moderado | Alto (pero sujeto a cuotas/latencia) |

**Puntos a Analizar:**
*   ¬øLa calidad de los embeddings de BGE-M3 es comparable a Gemini para esta tarea espec√≠fica (agrupar oraciones por tema)?
*   ¬øEl rendimiento del modelo local es aceptable en Vercel Serverless (memoria, tiempos de carga)?
*   ¬øSe expone al usuario como dos opciones o se auto-selecciona seg√∫n disponibilidad del modelo local?

---

### M-004: PII en Modo Simple ‚Äî No Auto-Activar

**Estado Actual:** El `useSmartConfig` puede activar `maskPii` autom√°ticamente. La protecci√≥n de datos PII implica procesamiento del texto (regex o LLM) que contradice el principio de "zero procesamiento extra" del modo simple.

**Propuesta:**
*   En Modo Simple, la m√°scara PII debe estar **DESACTIVADA por defecto**.
*   Si el sistema detecta que el documento podr√≠a contener datos sensibles, debe **advertir al usuario antes de lanzar** la ingesta, no activarlo silenciosamente.
*   El usuario decide: subir sin protecci√≥n (r√°pido) o activar la protecci√≥n (lo que podr√≠a cambiar el modo autom√°ticamente al Intermedio).

**Objetivo:** Coherencia total con el principio de que Modo Simple = zero LLM, zero procesamiento extra.

---

### M-005: Dise√±ar un Modo Intermedio

**Estado Actual:** Solo existen dos modos: Simple (sin control, auto-configurado) y Experto (control total). Hay un salto funcional enorme entre ambos.

**Propuesta:** Crear un **Modo Intermedio** que:
*   Permita **auto-detecci√≥n de industria** (v√≠a LLM, a diferencia del Simple que lo lee del contexto).
*   Permita activar/desactivar PII con un toggle visible.
*   Use fragmentaci√≥n **Simple (mec√°nica)** por defecto, pero permita cambiarla.
*   **No** active funcionalidades Premium (Vision, GraphRAG, Cognitive) ‚Äî esas se reservan para Experto.
*   Sea la opci√≥n recomendada para usuarios con experiencia media que quieran un poco m√°s de control sin la complejidad total del Experto.

**Nota:** Evaluar si este modo se integra como una opci√≥n dentro del modal actual o si requiere un flujo separado.

---

### M-006: Nivel Alto (Ag√©ntico) = Funcionalidad Premium

**Estado Actual:** El nivel Alto est√° disponible para cualquier usuario que active el Modo Experto, sin restricciones.

**Propuesta:**
*   El nivel **Alto** debe estar **vinculado al plan de facturaci√≥n** del tenant.
*   Solo los tenants con plan "Premium" (por definir) podr√°n activar este nivel.
*   Si un tenant sin plan Premium intenta usarlo, se le muestra un mensaje de upselling o se degrada autom√°ticamente al nivel Medio.
*   La definici√≥n exacta de "Premium" (precio, l√≠mites, features incluidas) se determinar√° en una fase posterior.

**Alcance Premium (a definir):**
*   Chunking Alto (Ag√©ntico)
*   Vision (An√°lisis visual de im√°genes/planos)
*   Contexto Cognitivo (micro-res√∫menes por fragmento)
*   GraphRAG (extracci√≥n de entidades y relaciones)

## 7. Estandarizaci√≥n y Consolidaci√≥n Arquitect√≥nica (Actualizaci√≥n v1.1)

Como parte de la mejora continua y deuda t√©cnica (FASE 199), se ha realizado una reestructuraci√≥n profunda del c√≥digo de ingesta para garantizar escalabilidad y consistencia.

### 7.1 Estandarizaci√≥n de Campos (Naming)
*   **Campo `sizeBytes`**: Se ha estandarizado el nombre del campo que representa el tama√±o del archivo de `fileSize` a `sizeBytes` en todo el monorepo (Schemas, servicios, scripts y logs de auditor√≠a).
*   **Raz√≥n**: Consistencia con los est√°ndares de la plataforma core y prevenci√≥n de ambig√ºedades en el manejo de unidades.

### 7.2 Consolidaci√≥n en `src/services/ingest/`
Se han movido los servicios orquestadores a un subdirectorio dedicado, eliminando servicios "hu√©rfanos" en la ra√≠z de `src/services/`:

*   `IngestService.ts`: Orquestador principal del pipeline RAG.
*   `IngestWorkerService.ts`: Gesti√≥n de colas as√≠ncronas (BullMQ).
*   **Impacto**: Mejora la modularidad y facilita la navegaci√≥n para desarrolladores.

### 7.3 Arquitectura de Componentes (v1.1)

```mermaid
graph TD
    A[API Routes / Use Cases] --> B[IngestService]
    B --> C[IngestPreparer]
    B --> D[IngestAnalyzer]
    B --> E[IngestIndexer]
    B --> F[IngestWorkerService]
    F --> G[BullMQ Queue]
```

### 7.4 Verificaci√≥n de Integridad
*   **Build**: 100% limpio (0 errores de TypeScript).
*   **Auditor√≠a**: Se han verificado las capas de seguridad (Guardian) y observabilidad para alinearlas con la nueva estructura.

---
*Este informe ha sido generado para la auditor√≠a de la FASE 199. √öltima actualizaci√≥n: 20 de febrero de 2026 (v1.1).*
