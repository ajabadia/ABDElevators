# ASPECTOS CRÃTICOS ADICIONALES (No Auditabilidad)

Estos son los gaps que van mÃ¡s allÃ¡ de auditorÃ­a pero son igual de crÃ­ticos para una plataforma SaaS enterprise.

---

## 1. PERFORMANCE & CACHING (CRÃTICO - Escalabilidad)

### Estado Actual
- âŒ **Sin cachÃ© distribuida**: MongoDB directo en cada query
- âŒ **Sin Redis**: Cada bÃºsqueda vectorial va a base de datos
- âŒ **SLA de RAG: 1000ms**, pero sin cache fallback
- âŒ **Rate limiting en memoria**: Ephemeral en Vercel Edge, se pierde entre despliegues
- âŒ **Sin connection pooling**: Cada request abre conexiÃ³n MongoDB

### QuÃ© Hace Falta

**Redis para:**
1. **Cache de bÃºsquedas RAG** (TTL 1h)
   - Query hash â†’ cached results
   - Evita recÃ¡lculos de embeddings
   - P95 baja de 1000ms a 200ms

2. **Rate limiting persistente**
   - Hoy: solo memoria local (pierde tras redeploy)
   - Necesario: Redis para rastrear entre instancias Vercel
   - Importante para proteger APIs de abuso

3. **Session store**
   - NextAuth actualmente usa default (cookies/DB)
   - Redis para microsegundos de latencia

4. **Job queue (Bull/BullMQ)**
   - Procesamiento async (PDFs grandes, anÃ¡lisis RAG pesado)
   - Background jobs (archivado automÃ¡tico, cron jobs)
   - Webhooks con retry logic

**Estrategia de caching:**
```
Level 1: In-Memory (supabase/postgres client-side) - 10ms
Level 2: Redis (distributed) - 50-100ms
Level 3: MongoDB (source of truth) - 500ms+

Invalidation:
- En cambios de prompt â†’ evict related caches
- Soft-delete â†’ evict del cache inmediatamente
- TTL automÃ¡tico: 1h para RAG, 6h para config
```

### Impacto
- **Performance**: P95 queries 1000ms â†’ 200ms
- **Cost**: Menos requests a MongoDB Atlas
- **Escalabilidad**: Soporta 10x mÃ¡s usuarios sin saturarse

### ImplementaciÃ³n
- LibrerÃ­a: Redis de Upstash (serverless)
- Pattern: Cache-aside con invalidation explÃ­cita
- Testing: Verificar TTL, invalidation, fallback

---

## 2. ERROR HANDLING & RESILIENCE (ALTO - Confiabilidad)

### Estado Actual
- ğŸŸ¡ AppError existe pero **sin retry logic**
- ğŸŸ¡ Sin circuit breakers (si Gemini falla, falla todo)
- ğŸŸ¡ Sin fallback strategies
- ğŸŸ¡ Sin timeout controls en llamadas LLM
- ğŸŸ¡ SLA violations no tienen penalty o degradation

### QuÃ© Hace Falta

**1. Circuit Breaker Pattern:**
```javascript
// Hoy: cada error Gemini replica
const response = await callGeminiMini(prompt, tenantId)

// DeberÃ­a: detectar patrÃ³n de fallos
const circuitBreaker = new CircuitBreaker({
  failureThreshold: 5,      // 5 errores
  resetTimeout: 60000,      // 1 min
  volumeThreshold: 100      // en Ãºltimas 100 requests
})

const response = await circuitBreaker.execute(
  () => callGeminiMini(prompt, tenantId)
)

// Si circuit abierto â†’ fallback a modelo cached o respuesta genÃ©rica
```

**2. Retry Logic con Exponential Backoff:**
```javascript
// Para operaciones no-idempotent (Stripe webhooks)
const result = await retryWithBackoff(
  () => applyPaymentChange(tenantId),
  {
    maxAttempts: 3,
    delay: 100,
    multiplier: 2,
    jitter: true
  }
)

// 1er intento: 100ms
// 2do intento: 200ms + jitter
// 3er intento: 400ms + jitter
```

**3. Timeout Controls:**
```javascript
// Hoy: llamadas Gemini sin timeout explÃ­cito
// DeberÃ­a:
const geminiPromise = callGeminiMini(prompt, tenantId)
const timeoutPromise = new Promise((_, reject) =>
  setTimeout(() => reject('Timeout'), 5000) // 5 seg max
)

const response = await Promise.race([geminiPromise, timeoutPromise])
```

**4. Degradation Strategies:**
```javascript
// Si RAG falla â†’ fallback a bÃºsqueda simple
try {
  results = await hybridSearch(query, tenantId)
} catch {
  results = await fallbackSimpleSearch(query, tenantId)
  // Log como WARN
}

// Si Stripe webhook fallido 3 veces â†’ mark as RETRY_LATER
// Admin puede re-procesar manualmente despuÃ©s
```

**5. Dead Letter Queue:**
```javascript
// Para eventos que fallan sistemÃ¡ticamente
const failedEvents = await db.collection('deadletter').find({
  status: 'PENDING',
  attemptCount: {$gte: 3}
}).toArray()

// Admin puede ver, investigar, retroceder o resolver
```

### Impacto
- **Reliability**: 99.5% â†’ 99.9% uptime
- **User experience**: Degradation elegante vs. crash
- **Debugging**: Clear logs de quÃ© fallÃ³ y por quÃ©

### ImplementaciÃ³n
- LibrerÃ­as: `opossum` (circuit breaker), `async-retry`, `p-retry`
- IntegraciÃ³n: En servicios crÃ­ticos (LLM, billing, auth)

---

## 3. MONITORING & OBSERVABILITY (ALTO - Operaciones)

### Estado Actual
- ğŸŸ¡ Logs bÃ¡sicos en MongoDB
- âŒ **Sin mÃ©tricas de performance** (P50, P95, P99)
- âŒ **Sin tracing distribuido** (imposible seguir request por sistema)
- âŒ **Sin alertas proactivas** de anomalÃ­as
- âŒ **Sin dashboards de health**
- âŒ **Sin SLO tracking** (promises vs. reality)

### QuÃ© Hace Falta

**1. Metrics Collection (Prometheus/StatsD format):**
```javascript
// Ejemplo en cada operaciÃ³n crÃ­tica
const timer = metrics.timer('rag.search.duration')
try {
  results = await hybridSearch(query, tenantId)
  metrics.gauge('rag.search.results', results.length)
  metrics.increment('rag.search.success')
} catch (error) {
  metrics.increment('rag.search.error', {error: error.type})
} finally {
  timer.stop()
}

// Resultado: histogram de latencias
// P50: 150ms, P95: 800ms, P99: 1200ms
```

**2. Distributed Tracing (OpenTelemetry):**
```javascript
// Request entra al middleware
const span = tracer.startSpan('api.request')
span.setAttribute('http.method', request.method)
span.setAttribute('http.url', request.url)

// Dentro del servicio
const ragsSpan = tracer.startSpan('rag.search', {parent: span})
ragsSpan.setAttribute('query', query)

// Dentro de Gemini call
const geminiSpan = tracer.startSpan('gemini.call', {parent: ragsSpan})
geminiSpan.setAttribute('model', 'gemini-2.0-flash')

// Resultado: waterfall visual de dÃ³nde pasÃ³ el tiempo
// middleware (10ms) â†’ service (20ms) â†’ rag (800ms) â†’ gemini (750ms)
```

**3. Alerting Rules:**
```javascript
// 1. SLA violations
alert.rule('rag.search.p95 > 1000ms', {
  severity: 'WARNING',
  action: 'notify_team',
  threshold: '5 min'
})

// 2. Error rate spike
alert.rule('rag.search.error_rate > 5%', {
  severity: 'CRITICAL',
  action: 'page_oncall',
  threshold: '1 min'
})

// 3. Resource exhaustion
alert.rule('mongodb.connection_pool > 80%', {
  severity: 'WARNING',
  action: 'scale_up',
  threshold: '2 min'
})

// 4. Anomaly detection
alert.rule('rag.search.latency_zscore > 3', {
  severity: 'INFO',
  action: 'log_and_investigate'
})
```

**4. Dashboard:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ABD RAG Platform - Health Dashboard      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Status: ğŸŸ¢ HEALTHY                       â”‚
â”‚ Uptime: 99.95% (last 30 days)           â”‚
â”‚                                         â”‚
â”‚ API Latency (last 1h)                   â”‚
â”‚ â”œâ”€ P50:  120ms â–„â–„â–„â–„                     â”‚
â”‚ â”œâ”€ P95:  850ms â–„â–„â–„â–„â–„â–„â–„â–„                 â”‚
â”‚ â””â”€ P99: 1200ms â–„â–„â–„â–„â–„â–„â–„â–„â–„                â”‚
â”‚                                         â”‚
â”‚ Error Rate: 0.2% â–„                      â”‚
â”‚ Active Users: 42                         â”‚
â”‚                                         â”‚
â”‚ Database Connections: 18/50 â–„â–„â–„â–„â–„â–„      â”‚
â”‚ Redis Memory: 2.3GB / 5GB â–„â–„â–„â–„â–„         â”‚
â”‚                                         â”‚
â”‚ Top Slow Endpoints:                     â”‚
â”‚ 1. POST /api/pedidos/analyze (950ms)   â”‚
â”‚ 2. GET /api/search (820ms)              â”‚
â”‚ 3. POST /api/prompts/request (750ms)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Impacto
- **Visibility**: Saber quÃ© estÃ¡ pasando en producciÃ³n
- **Debugging**: Traces para reproducir issues
- **Optimization**: Data-driven performance improvements
- **SLO tracking**: Probar cumplimiento con clientes

### ImplementaciÃ³n
- LibrerÃ­as: `@opentelemetry/*`, StatsD client
- Backend: Vercel Analytics, DataDog, New Relic, o self-hosted Prometheus
- Dashboards: Grafana + Prometheus

---

## 4. API SECURITY & RATE LIMITING (ALTO - Seguridad)

### Estado Actual
- ğŸŸ¡ Rate limiting basic en middleware (no persistente)
- âŒ **Sin API key management** (users no pueden crear keys)
- âŒ **Sin oauth scope control** (todo-o-nada)
- âŒ **Sin IP allowlisting**
- âŒ **Sin DDoS protection**
- âŒ **Sin webhook signature verification** (parcial en Stripe)

### QuÃ© Hace Falta

**1. API Key Management:**
```typescript
// Usuarios TECNICO+ deben poder crear keys para integrar
// POST /api/users/api-keys
{
  name: "IntegraciÃ³n ERP",
  scopes: ["read:pedidos", "write:pedidos", "read:resultados"],
  expiresIn: "90d",
  ipAllowlist: ["192.168.1.1", "10.0.0.0/8"]
}

// Respuesta: {key: "abd_rag_sk_abc123..."}
// Guardar como hash SHA256 en DB (nunca guardar en claro)
// Audit trail: quiÃ©n creÃ³, cuÃ¡ndo, desde dÃ³nde, cambios, rotaciÃ³n

// En utilizaciÃ³n:
// GET /api/pedidos?key=abd_rag_sk_abc123...
// Verificar: key vÃ¡lida, no expirada, IP en allowlist, scope correcto
```

**2. Oauth-style Scope Control:**
```
read:pedidos
write:pedidos
read:documentos
read:resultados
admin:config (solo ADMIN)
audit:logs (solo COMPLIANCE)
```

**3. Advanced Rate Limiting:**
```javascript
// Hoy: simple counter
// DeberÃ­a: multi-level + quotas

// Nivel 1: Por API key (Redis)
rateLimit('api_key:abc123', {
  limit: 1000,           // requests per hour
  burst: 50,             // max simultaneous
  window: 3600 * 1000    // 1 hour
})

// Nivel 2: Por tenant (Redis)
rateLimit('tenant:xyz', {
  limit: 10000,
  window: 3600 * 1000
})

// Nivel 3: Global (protecciÃ³n contra DDoS)
rateLimit('global', {
  limit: 100000,
  window: 60 * 1000
})

// Si excede, devolver 429 con Retry-After header
// "You have 950 requests remaining. Quota resets in 47 minutes."
```

**4. IP Allowlisting & Blocking:**
```javascript
// Per-tenant configuration
const config = {
  ipAllowlist: ['1.2.3.4', '5.6.7.0/24'],  // Si hay, SOLO estos IPs
  ipBlocklist: ['8.9.10.11'],               // Siempre bloquear
  requireMFA: true,
  requireVPN: false  // Empresa con VPN corporativo
}

// En middleware:
if (config.ipAllowlist && !config.ipAllowlist.includes(ip)) {
  return 403 // Forbidden
}
```

**5. DDoS Protection:**
```javascript
// IntegraciÃ³n con Cloudflare / AWS WAF
// Detectar patrones:
// - 1000+ requests desde mismo IP en 10 seg â†’ bloquear
// - 50+ 404s desde mismo IP â†’ challenge CAPTCHA
// - File traversal attempts â†’ bloquear inmediatamente
```

### Impacto
- **Security**: Control granular de acceso
- **Audit**: Trazabilidad de quiÃ©n accediÃ³ quÃ©
- **Protection**: DDoS, abuso de API, fuerza bruta
- **Scale**: Usuarios pueden integrar sin acceso directo a DB

### ImplementaciÃ³n
- Rate limiting: Redis + estrategia multi-level
- API keys: Hash con salto, expiraciÃ³n, rotation
- DDoS: Cloudflare Workers o AWS WAF

---

## 5. DATA CONSISTENCY & TRANSACTIONS (ALTO - Integridad)

### Estado Actual
- ğŸŸ¡ Zod validation en entrada
- âŒ **Sin transacciones ACID** (insertos sin rollback)
- âŒ **Sin constraint enforcement** (unique, foreign keys)
- âŒ **Sin idempotency keys** (duplicados si request retried)
- âŒ **Sin event sourcing** (cambios no rastreables)

### QuÃ© Hace Falta

**1. MongoDB Transactions (ACID):**
```javascript
// Hoy: inserts independientes (si uno falla, inconsistencia)
const pedidoId = await pedidosCol.insertOne(pedido)
await riesgosCol.insertOne(riesgos)  // Â¿QuÃ© pasa si falla?

// DeberÃ­a:
const session = await db.client.startSession()
try {
  await session.withTransaction(async () => {
    const result = await pedidosCol.insertOne(pedido, {session})
    const pedidoId = result.insertedId
    
    await riesgosCol.insertOne({
      pedidoId,
      ...riesgos
    }, {session})
    
    // TODO mÃ¡s operaciones...
  })
} catch (error) {
  // Todo se rollback automÃ¡tico
  throw error
}
```

**2. Idempotency Keys:**
```javascript
// Para operaciones crÃ­ticas (Stripe, cambios de estado)

// Cliente envÃ­a:
POST /api/pedidos/123/transition
Idempotency-Key: 5f8c2a1f-d3e4-4b6c-9a2d-1e3f5g7h9j0k
Body: {status: "COMPLETED", reason: "..."}

// Backend:
const idempotencyKey = req.headers['idempotency-key']
const cached = await idempotencyCache.get(idempotencyKey)

if (cached) {
  return cached  // Return mismo resultado, no re-ejecutar
}

// Ejecutar operaciÃ³n
const result = await pedidosCol.updateOne(...)

// Cachear por 24h (Stripe SLA)
await idempotencyCache.set(idempotencyKey, result, '24h')

return result
```

**3. Unique Constraints:**
```typescript
// En schemas Mongo:
await db.collection('prompts').createIndex(
  {tenantId: 1, key: 1},
  {unique: true}  // No 2 prompts con mismo key en tenant
)

await db.collection('usuarios').createIndex(
  {email: 1},
  {unique: true, sparse: true}  // Email Ãºnico globalmente
)
```

**4. Event Sourcing Pattern (opcional pero powerful):**
```javascript
// En lugar de sobrescribir estados, registrar eventos
await eventsCol.insertOne({
  eventId: UUID(),
  aggregateId: pedidoId,
  eventType: 'PEDIDO_CREADO' | 'PEDIDO_ANALIZADO' | 'RIESGO_DETECTADO',
  data: {...},
  timestamp: new Date(),
  version: 1
})

// Reconstruir estado actual sumando eventos
const estado = await reconstructState(pedidoId)
// = {status: 'COMPLETED', riesgos: [...], ...}
```

### Impacto
- **Data integrity**: No inconsistencias, todos los datos completos
- **Idempotency**: Safe retries, no duplicados
- **Auditability**: Event log de quÃ© pasÃ³ (complementa audit trail)

### ImplementaciÃ³n
- MongoDB sessions para transacciones
- Redis para idempotency cache
- Event store (MongoDB capped collection o Kafka)

---

## 6. SCALABILITY & MULTI-TENANCY (MEDIO - Arquitectura)

### Estado Actual
- ğŸŸ¡ Multi-tenant existe pero **sin resource isolation**
- ğŸŸ¡ Un tenant "ruidoso" afecta a otros (noisy neighbor)
- âŒ **Sin rate limiting por tenant**
- âŒ **Sin quota management**
- âŒ **Sin data sharding** (un tenant > 1GB de datos)

### QuÃ© Hace Falta

**1. Resource Isolation:**
```javascript
// Hoy: todos los tenants comparten pool MongoDB
// DeberÃ­a:

// OpciÃ³n A: Database per tenant (mÃ¡ximo aislamiento)
const dbConnection = await connectDB(`abd-rag-${tenantId}`)

// OpciÃ³n B: Collections namespaced (mÃ¡s comÃºn)
const collection = db.collection(`${tenantId}_pedidos`)

// OpciÃ³n C: Attribute-based (actual, pero mejorable)
// AÃ±adir Ã­ndice compuesto para rapidez
await db.collection('pedidos').createIndex({
  tenantId: 1,
  creado: -1
})
```

**2. Tenant Quotas:**
```typescript
// En TenantService
interface TenantQuota {
  maxStorageMB: 10000,
  maxDocuments: 5000,
  maxAPICallsPerDay: 50000,
  maxRAGSearchesPerDay: 10000,
  maxConcurrentUsers: 100
}

// En operaciones crÃ­ticas:
const currentStorage = await getTenantStorageUsage(tenantId)
if (currentStorage + fileSize > quota.maxStorageMB) {
  throw new Error('Storage quota exceeded')
}
```

**3. Fair Resource Allocation:**
```javascript
// En rate limiter, ser justo entre tenants
const tenantLimit = baseLimit / activeTenantsCount

// Si hay 10 tenants activos, cada uno obtiene 1000/10 = 100 req/min
// Si hay 2, cada uno obtiene 1000/2 = 500 req/min

// Algoritmo de backpressure:
if (loadPerTenant > 80%) {
  // Desacelerar progresivamente en lugar de bloquear
  request.timeout = timeout * 1.5
}
```

### Impacto
- **Fairness**: Un tenant no monopoliza recursos
- **Sustainability**: Platform soporta mÃ¡s tenants
- **Billing accuracy**: Quotas = facturaciÃ³n justa

### ImplementaciÃ³n
- Namespace en colecciones
- Quotas por tenant en config
- Rate limiting avanzado (ver secciÃ³n 4)

---

## 7. DEPLOYMENT & TESTING (MEDIO - DevOps)

### Estado Actual
- ğŸŸ¡ Vercel deployment automÃ¡tico en `main`
- âŒ **Sin canary deployments** (todos 100% a la vez)
- âŒ **Sin rollback automation**
- âŒ **Sin blue-green deployments**
- âŒ **Sin testing pre-deployment** (unit + integration)
- âŒ **Sin synthetic monitoring** (uptime checks)

### QuÃ© Hace Falta

**1. Canary Deployments:**
```
VersiÃ³n actual (v1): 100% de traffic
Nueva versiÃ³n (v2): 0% de traffic

Paso 1: v2 = 5% traffic, monitorear mÃ©tricas
        â”œâ”€ Error rate < 0.5%?
        â”œâ”€ P95 latency < 1200ms?
        â””â”€ Alertas crÃ­ticas?

Paso 2: v2 = 25% traffic, monitorear

Paso 3: v2 = 50% traffic, monitorear

Paso 4: v2 = 100% traffic

Si en cualquier paso falla â†’ rollback automÃ¡tico a v1
```

**2. Testing Strategy:**
```
Unit Tests (fast, 100% coverage critical paths):
  â”œâ”€ PromptService.updatePrompt() â†’ creates ApprovalRequest
  â”œâ”€ SoftDelete logic â†’ document stays in DB
  â”œâ”€ Hash validation â†’ detects tampering
  â””â”€ Rate limiting â†’ blocks after limit

Integration Tests (medio, critical flows):
  â”œâ”€ User signup â†’ email sent â†’ can login
  â”œâ”€ Prompt change â†’ approval flow â†’ applied
  â”œâ”€ Pedido analysis â†’ risk detection â†’ logged
  â””â”€ Stripe webhook â†’ subscription updated â†’ user can access

E2E Tests (slow, critical user journeys):
  â”œâ”€ Admin: upload doc â†’ search â†’ analyze â†’ export
  â”œâ”€ Tecnico: create ticket â†’ resolved
  â””â”€ Enterprise: bulk API consumption â†’ billed correctly

Load Tests (pre-deployment):
  â”œâ”€ 100 concurrent users â†’ P95 < 1000ms?
  â”œâ”€ 1000 RAG searches/min â†’ cache hit rate > 70%?
  â””â”€ Database 500 concurrent connections â†’ stable?
```

**3. Synthetic Monitoring (constant health checks):**
```javascript
// Cada 1 minuto, probar endpoints crÃ­ticos
cronJob.every('1 minute', async () => {
  const results = {
    api_health: await fetch('/api/health'),
    rag_search: await fetch('/api/tecnico/rag-chat', {
      method: 'POST',
      body: JSON.stringify({question: 'test query'})
    }),
    auth: await fetch('/api/auth/session'),
    database: await db.collectionusers.findOne({})
  }
  
  // Si alguno falla â†’ page oncall
  if (results.some(r => !r.success)) {
    await alerting.page('Synthetic monitor failed')
  }
})
```

### Impacto
- **Safety**: Despliegues sin downtime
- **Rollback**: Si algo sale mal, revert automÃ¡tico
- **Visibility**: Uptime garantizado y probado

### ImplementaciÃ³n
- GitHub Actions para CI/CD
- Vercel Deployments API para canary
- Jest para unit tests, Cypress para E2E

---

## 8. DOCUMENTATION & KNOWLEDGE MANAGEMENT (BAJO - DX)

### Estado Actual
- ğŸŸ¡ Roadmap interno en doc
- âŒ **Sin API documentation** (OpenAPI/Swagger)
- âŒ **Sin SDK** (clientes tienen que escribir HTTP calls)
- âŒ **Sin runbooks** (cÃ³mo responder incidentes)
- âŒ **Sin architecture decision records (ADR)**

### QuÃ© Hace Falta

**1. API Documentation (OpenAPI 3.0):**
```yaml
openapi: 3.0.0
info:
  title: ABD RAG API
  version: 1.0.0

paths:
  /api/tecnico/rag-chat:
    post:
      summary: Execute RAG query
      tags: [RAG]
      parameters:
        - name: x-api-key
          in: header
          required: true
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                question:
                  type: string
      responses:
        200:
          description: Success
          content:
            application/json:
              schema:
                type: object
                properties:
                  answer: {type: string}
                  documents: {type: array}
                  trace: {type: array}
```

Generar Swagger UI automÃ¡ticamente.

**2. SDK (TypeScript/Python):**
```typescript
// npm install abd-rag-sdk
import {ABDRagClient} from 'abd-rag-sdk'

const client = new ABDRagClient({
  apiKey: 'abd_rag_sk_...',
  endpoint: 'https://api.rag.abd.com'
})

const response = await client.rag.chat({
  question: 'Â¿QuÃ© modelos de motor tiene el edificio?'
})

console.log(response.answer)
console.log(response.documents) // Con citas
```

**3. Runbooks (incident response):**
```markdown
# Incident: High Error Rate in RAG Search

## Detection
- Alert: `rag.search.error_rate > 5%` triggered at 14:35 UTC
- Dashboard: https://...

## Investigation
1. Check Gemini API status â†’ https://status.google.com
2. Check MongoDB connection pool â†’ `metrics.mongodb.connections`
3. Check Redis memory â†’ `redis-cli INFO memory`
4. Check recent deployments â†’ GitHub Actions

## Response
- If Gemini down: Route to fallback cached results, notify users
- If DB down: Page database oncall, initiate failover
- If memory leak: Restart pod (Vercel restart deployment)

## Rollback
- If caused by recent deploy: `vercel rollback <deployment-id>`
- Monitor for 10 minutes post-action

## Post-Incident
- Document in incident log
- Schedule RCA meeting
- Create ticket for prevention
```

**4. Architecture Decision Records (ADR):**
```markdown
# ADR-001: Choose MongoDB for Primary Storage

## Context
We needed a database that:
- Scales horizontally (multi-tenant)
- Supports complex queries + vector search
- Has good Node.js integration

## Decision
Use MongoDB Atlas with vector search indexes (Atlas Vector Search)

## Consequences
- Positive: Good performance for RAG, flexible schema
- Negative: Extra cost for vector search, new index type to learn
- Mitigations: Caching layer to reduce queries

## Alternatives Considered
- PostgreSQL + pgvector: Better for relational, overkill for unstructured
- Pinecone: Pure vector DB, not good for transactional data
- Elasticsearch: Better full-text, less native support for vectors

Status: ACCEPTED
Date: 2025-01-28
```

### Impacto
- **Developer Experience**: FÃ¡cil para terceros integrar
- **Knowledge Transfer**: Runbooks para nuevos en el equipo
- **Architecture clarity**: Decisiones registradas, justificadas

### ImplementaciÃ³n
- OpenAPI: `@nestjs/swagger` o manual
- Runbooks: Markdown en wiki o Notion
- ADRs: Folder en repo `/docs/adr/`

---

## PRIORIZACIÃ“N FINAL (Roadmap 12 semanas)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SEMANAS 1-12: Full Stack         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚ SEMANAS 1-5: AUDITABILITY (anterior)    â”‚
â”‚ (9 gaps, Foundation phase)              â”‚
â”‚                                         â”‚
â”‚ SEMANA 6: CACHING & REDIS               â”‚
â”‚ â”œâ”€ Redis setup (Upstash)                â”‚
â”‚ â”œâ”€ Cache layer para RAG                 â”‚
â”‚ â”œâ”€ Rate limiting persistente            â”‚
â”‚ â””â”€ Testing & validation                 â”‚
â”‚ â±ï¸ Effort: 40 horas                     â”‚
â”‚ ğŸ“Š Impact: P95 latency 1000â†’200ms      â”‚
â”‚                                         â”‚
â”‚ SEMANA 7: ERROR HANDLING & RESILIENCE   â”‚
â”‚ â”œâ”€ Circuit breaker (Gemini calls)       â”‚
â”‚ â”œâ”€ Retry logic + backoff                â”‚
â”‚ â”œâ”€ Timeout controls                     â”‚
â”‚ â”œâ”€ Degradation strategies               â”‚
â”‚ â””â”€ Dead letter queue                    â”‚
â”‚ â±ï¸ Effort: 35 horas                     â”‚
â”‚ ğŸ“Š Impact: Uptime 99.5%â†’99.9%          â”‚
â”‚                                         â”‚
â”‚ SEMANA 8: MONITORING & OBSERVABILITY    â”‚
â”‚ â”œâ”€ OpenTelemetry integration            â”‚
â”‚ â”œâ”€ Prometheus metrics                   â”‚
â”‚ â”œâ”€ Alerting rules                       â”‚
â”‚ â”œâ”€ Health dashboard                     â”‚
â”‚ â””â”€ PagerDuty integration                â”‚
â”‚ â±ï¸ Effort: 45 horas                     â”‚
â”‚ ğŸ“Š Impact: Visibility + proactive ops  â”‚
â”‚                                         â”‚
â”‚ SEMANA 9: API SECURITY & RATE LIMITING  â”‚
â”‚ â”œâ”€ API key management                   â”‚
â”‚ â”œâ”€ Advanced rate limiting (multi-level) â”‚
â”‚ â”œâ”€ IP allowlisting/blocking             â”‚
â”‚ â”œâ”€ DDoS protection (Cloudflare)         â”‚
â”‚ â””â”€ Audit trail para API keys            â”‚
â”‚ â±ï¸ Effort: 40 horas                     â”‚
â”‚ ğŸ“Š Impact: Security hardening          â”‚
â”‚                                         â”‚
â”‚ SEMANA 10: DATA CONSISTENCY             â”‚
â”‚ â”œâ”€ MongoDB transactions (ACID)          â”‚
â”‚ â”œâ”€ Idempotency keys                     â”‚
â”‚ â”œâ”€ Unique constraints                   â”‚
â”‚ â””â”€ Event sourcing (optional)            â”‚
â”‚ â±ï¸ Effort: 30 horas                     â”‚
â”‚ ğŸ“Š Impact: Data integrity assured      â”‚
â”‚                                         â”‚
â”‚ SEMANA 11: TESTING & DEPLOYMENT         â”‚
â”‚ â”œâ”€ Unit tests (+80% coverage)           â”‚
â”‚ â”œâ”€ Integration tests (critical paths)   â”‚
â”‚ â”œâ”€ E2E tests (user journeys)            â”‚
â”‚ â”œâ”€ Load tests (pre-deployment)          â”‚
â”‚ â”œâ”€ Canary deployments                   â”‚
â”‚ â””â”€ Synthetic monitoring                 â”‚
â”‚ â±ï¸ Effort: 50 horas                     â”‚
â”‚ ğŸ“Š Impact: Safe deployments, zero issuesâ”‚
â”‚                                         â”‚
â”‚ SEMANA 12: DOCUMENTATION & RUNBOOKS     â”‚
â”‚ â”œâ”€ OpenAPI 3.0 spec                     â”‚
â”‚ â”œâ”€ SDK (TypeScript)                     â”‚
â”‚ â”œâ”€ Runbooks (incident response)         â”‚
â”‚ â”œâ”€ ADRs (architecture decisions)        â”‚
â”‚ â””â”€ Training materials                   â”‚
â”‚ â±ï¸ Effort: 35 horas                     â”‚
â”‚ ğŸ“Š Impact: Better DX, easier onboardingâ”‚
â”‚                                         â”‚
â”‚ TOTAL: 5 + 40 + 35 + 45 + 40 + 30 + 50 + 35 = 280 horas
â”‚        â‰ˆ 1 dev full-time x 7 semanas
â”‚        â‰ˆ 2 devs x 3.5 semanas
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

POST-IMPLEMENTATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Week 13+: Open beta with early customers
Week 16+: Full production launch
```

---

## MATRIX DE IMPACTO vs. ESFUERZO

```
          IMPACTO ALTO
              â–²
              â”‚
              â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     â”‚ AUDITABILITYâ”‚  (9 gaps)
              â”‚     â”‚ CACHING     â”‚  280h total
              â”‚     â”‚ TESTING     â”‚
              â”‚     â”‚ MONITORING  â”‚
              â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     â”‚ RESILIENCE  â”‚
              â”‚     â”‚ SECURITY    â”‚
              â”‚     â”‚ CONSISTENCY â”‚
              â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                    â”‚SCALABILITY  â”‚
              â”‚                    â”‚DOCUMENTATIONâ”‚
              â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º ESFUERZO BAJO

RECOMENDACIÃ“N:
- Haz TODO lo de la esquina superior izquierda (AUDITABILITY + CACHING)
- Es el mÃ¡ximo impacto + esfuerzo razonable
- DespuÃ©s, sÃ­, rest de aspectos, pero auditability + caching primero
```

---

## CHECKLIST "PRODUCTION-READY"

```
âœ… Auditability:
  â”œâ”€ Append-only audit logs
  â”œâ”€ Soft-delete everywhere
  â”œâ”€ 4-eyes approval
  â”œâ”€ Digital signatures
  â””â”€ Compliance dashboards

âœ… Performance:
  â”œâ”€ Redis caching
  â”œâ”€ Connection pooling
  â”œâ”€ P95 latency < 500ms
  â””â”€ Cache hit rate > 60%

âœ… Reliability:
  â”œâ”€ Circuit breakers
  â”œâ”€ Retry logic
  â”œâ”€ Graceful degradation
  â””â”€ 99.9% uptime target

âœ… Security:
  â”œâ”€ API key management
  â”œâ”€ Advanced rate limiting
  â”œâ”€ IP allowlisting
  â””â”€ DDoS protection

âœ… Observability:
  â”œâ”€ Distributed tracing
  â”œâ”€ Prometheus metrics
  â”œâ”€ Alert rules
  â””â”€ Health dashboards

âœ… Testing:
  â”œâ”€ Unit tests (80%+ coverage)
  â”œâ”€ Integration tests
  â”œâ”€ E2E tests
  â””â”€ Load tests

âœ… Data Integrity:
  â”œâ”€ ACID transactions
  â”œâ”€ Idempotency keys
  â”œâ”€ Unique constraints
  â””â”€ Event sourcing

âœ… Documentation:
  â”œâ”€ OpenAPI spec
  â”œâ”€ SDK published
  â”œâ”€ Runbooks
  â””â”€ ADRs
```

Si logras todo esto en 12 semanas, tienes una **plataforma SaaS enterprise-ready** lista para clientes grandes, auditorÃ­as de compliance, y escala a 1000+ usuarios.

