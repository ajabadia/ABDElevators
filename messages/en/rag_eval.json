{
  "title": "RAG Evaluation Dashboard",
  "subtitle": "Observability and response quality (Faithfulness, Relevance, Precision)",
  "judge_badge": "LLM Judge: Gemini 1.5 Pro",
  "metrics": {
    "faithfulness": "Faithfulness (Avg)",
    "relevance": "Answer Relevance",
    "precision": "Context Precision",
    "queries": "Evaluated Queries",
    "faithfulness_desc": "Answers based only on context",
    "relevance_desc": "Answer pertinence",
    "precision_desc": "Utility of retrieved context",
    "queries_desc": "Total analyzed traces"
  },
  "tabs": {
    "trends": "Quality Trends",
    "history": "Trace History"
  },
  "chart": {
    "title": "Metrics Evolution",
    "desc": "Daily tracking of faithfulness and relevance of responses",
    "faithfulness": "Faithfulness",
    "relevance": "Relevance"
  },
  "table": {
    "date": "Date",
    "query": "Query",
    "metrics": "Metrics (F|R|P)",
    "feedback": "Judge Feedback"
  },
  "playground": {
    "parameters": "Experiment Parameters",
    "model": "Generative Model",
    "temperature": "Creativity (Temp)",
    "chunk_settings": "Retrieval Settings",
    "chunk_size": "Chunk Size",
    "top_k": "Top-K (Ranking)",
    "run_btn": "Run Simulation",
    "no_results": "Enter a query and press run to see AI reasoning.",
    "faithfulness": "Faithfulness",
    "relevance": "Relevance",
    "precision": "Precision"
  }
}